{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7daee-1498-4d59-8dee-96ac99793670",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subarna/anaconda3/envs/news_classifier/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1244184 headlines\n",
      "Preprocessing texts...\n",
      ">>> Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [04:18<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering into 50 clusters...\n",
      "Reducing dimensions with IncrementalPCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    142\u001b[39m report_df = pd.DataFrame(report_rows)\n\u001b[32m    143\u001b[39m report_df.to_csv(\u001b[33m\"\u001b[39m\u001b[33mcluster_report.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[43mreport_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcluster_report.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCluster report saved as CSV and Excel.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    147\u001b[39m html_parts = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/news_classifier/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/news_classifier/lib/python3.12/site-packages/pandas/core/generic.py:2436\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2423\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2425\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2426\u001b[39m     df,\n\u001b[32m   2427\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2434\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2435\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/news_classifier/lib/python3.12/site-packages/pandas/io/formats/excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/news_classifier/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Stopwords --> \n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# LLM summarization\n",
    "from transformers import pipeline\n",
    "\n",
    "# Keyword extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"./abcnews-date-text.csv\")\n",
    "texts = df[\"headline_text\"].dropna().astype(str).tolist()\n",
    "print(\"Loaded\", len(texts), \"headlines\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Fast preprocessing\n",
    "# -----------------------------\n",
    "def fast_preprocess(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    return \" \".join([w for w in text.split() if w not in ENGLISH_STOP_WORDS and len(w) > 2])\n",
    "\n",
    "print(\"Preprocessing texts...\")\n",
    "processed_texts = [fast_preprocess(t) for t in texts]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Embeddings (Sentence-BERT, GPU if available)\n",
    "# -----------------------------\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\">>> Using device:\", device)\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "X = embedder.encode(processed_texts, batch_size=512, show_progress_bar=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Clustering (MiniBatchKMeans for scalability)\n",
    "# -----------------------------\n",
    "n_clusters = 50  # adjust based on how granular you want\n",
    "print(f\"Clustering into {n_clusters} clusters...\")\n",
    "clusterer = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10000, random_state=42)\n",
    "labels = clusterer.fit_predict(X)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Dimensionality reduction for visualization\n",
    "# -----------------------------\n",
    "print(\"Reducing dimensions with IncrementalPCA...\")\n",
    "ipca = IncrementalPCA(n_components=2, batch_size=10000)\n",
    "reduced = ipca.fit_transform(X)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Cluster analysis (keywords + summarization)\n",
    "# -----------------------------\n",
    "clustered = pd.DataFrame({\"headline\": texts, \"processed\": processed_texts, \"cluster\": labels})\n",
    "\n",
    "# TF-IDF for keywords\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(processed_texts)\n",
    "\n",
    "# Summarizer (GPU if available)\n",
    "summarizer = pipeline(\"summarization\", model=\"google/flan-t5-small\",\n",
    "                      device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def summarize_cluster(headlines):\n",
    "    if len(headlines) == 0:\n",
    "        return \"No data\"\n",
    "    text = \" \".join(headlines[:10])\n",
    "    summary = summarizer(text, max_length=30, min_length=5, do_sample=False)\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "cluster_labels = {}\n",
    "report_rows = []\n",
    "\n",
    "# Analyze only top clusters by size (saves summarization time)\n",
    "top_clusters = clustered[\"cluster\"].value_counts().head(15).index\n",
    "\n",
    "for c in top_clusters:\n",
    "    cluster_headlines = clustered[clustered.cluster == c].headline.tolist()\n",
    "    cluster_processed = clustered[clustered.cluster == c].processed.tolist()\n",
    "\n",
    "    # Keywords\n",
    "    cluster_texts = \" \".join(cluster_processed)\n",
    "    keywords = vectorizer.transform([cluster_texts]).toarray().flatten()\n",
    "    top_idx = keywords.argsort()[-10:][::-1]\n",
    "    top_words = [vectorizer.get_feature_names_out()[i] for i in top_idx]\n",
    "\n",
    "    # LLM summary\n",
    "    summary = summarize_cluster(cluster_headlines)\n",
    "\n",
    "    cluster_labels[c] = summary if summary != \"No data\" else \", \".join(top_words[:3])\n",
    "\n",
    "    report_rows.append({\n",
    "        \"cluster_id\": int(c),\n",
    "        \"size\": len(cluster_headlines),\n",
    "        \"top_keywords\": \", \".join(top_words),\n",
    "        \"summary\": summary,\n",
    "        \"sample_headlines\": \" | \".join(cluster_headlines[:5])\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# Step 7: Visualization with labels\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], hue=labels, palette=\"tab10\", s=10, legend=None)\n",
    "\n",
    "# Annotate cluster centers with summaries\n",
    "for c in top_clusters:\n",
    "    cluster_points = reduced[labels == c]\n",
    "    if len(cluster_points) == 0: continue\n",
    "    center = cluster_points.mean(axis=0)\n",
    "    plt.text(center[0], center[1], cluster_labels.get(c, str(c)),\n",
    "             fontsize=9, weight='bold',\n",
    "             bbox=dict(facecolor='white', alpha=0.6, boxstyle='round,pad=0.3'))\n",
    "\n",
    "plt.title(\"Headline Clusters (MiniBatchKMeans + IncrementalPCA)\")\n",
    "buf = BytesIO()\n",
    "plt.savefig(buf, format='png', bbox_inches='tight')\n",
    "buf.seek(0)\n",
    "main_plot_b64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 8: Export report (CSV/Excel/HTML)\n",
    "# -----------------------------\n",
    "report_df = pd.DataFrame(report_rows)\n",
    "report_df.to_csv(\"cluster_report.csv\", index=False)\n",
    "report_df.to_excel(\"cluster_report.xlsx\", index=False)\n",
    "print(\"\\nCluster report saved as CSV and Excel.\")\n",
    "\n",
    "html_parts = []\n",
    "html_parts.append(f\"<h1>Unsupervised NLP Cluster Report</h1>\")\n",
    "html_parts.append(f\"<p>Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC</p>\")\n",
    "html_parts.append(\"<h2>Main cluster visualization</h2>\")\n",
    "html_parts.append(f\"<img src=\\\"data:image/png;base64,{main_plot_b64}\\\" style=\\\"max-width:100%;height:auto;\\\"/>\")\n",
    "\n",
    "html_parts.append(\"<h2>Cluster details (Top 15 by size)</h2>\")\n",
    "for row in report_rows:\n",
    "    html_parts.append(f\"<div style='border:1px solid #ddd;padding:10px;margin:10px 0;border-radius:6px;'>\")\n",
    "    html_parts.append(f\"<h3>Cluster {row['cluster_id']} (size={row['size']}): {row['summary']}</h3>\")\n",
    "    html_parts.append(f\"<p><b>Top keywords:</b> {row['top_keywords']}</p>\")\n",
    "    html_parts.append(f\"<p><b>Sample headlines:</b> {row['sample_headlines']}</p>\")\n",
    "    html_parts.append(\"</div>\")\n",
    "\n",
    "html_content = \"\\n\".join(html_parts)\n",
    "with open(\"cluster_report.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<html><head><meta charset='utf-8'></head><body>\")\n",
    "    f.write(html_content)\n",
    "    f.write(\"</body></html>\")\n",
    "\n",
    "print(\"HTML report saved to cluster_report.html\")\n",
    "print(\"\\nPipeline complete ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35cdffda-84c2-47c2-83ff-e97cc2ccfbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster report saved as CSV and Excel.\n",
      "HTML report saved to cluster_report.html\n",
      "\n",
      "Pipeline complete ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49296/625560738.py:6: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  html_parts.append(f\"<p>Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC</p>\")\n"
     ]
    }
   ],
   "source": [
    "report_df.to_excel(\"cluster_report.xlsx\", index=False)\n",
    "print(\"\\nCluster report saved as CSV and Excel.\")\n",
    "\n",
    "html_parts = []\n",
    "html_parts.append(f\"<h1>Unsupervised NLP Cluster Report</h1>\")\n",
    "html_parts.append(f\"<p>Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC</p>\")\n",
    "html_parts.append(\"<h2>Main cluster visualization</h2>\")\n",
    "html_parts.append(f\"<img src=\\\"data:image/png;base64,{main_plot_b64}\\\" style=\\\"max-width:100%;height:auto;\\\"/>\")\n",
    "\n",
    "html_parts.append(\"<h2>Cluster details (Top 15 by size)</h2>\")\n",
    "for row in report_rows:\n",
    "    html_parts.append(f\"<div style='border:1px solid #ddd;padding:10px;margin:10px 0;border-radius:6px;'>\")\n",
    "    html_parts.append(f\"<h3>Cluster {row['cluster_id']} (size={row['size']}): {row['summary']}</h3>\")\n",
    "    html_parts.append(f\"<p><b>Top keywords:</b> {row['top_keywords']}</p>\")\n",
    "    html_parts.append(f\"<p><b>Sample headlines:</b> {row['sample_headlines']}</p>\")\n",
    "    html_parts.append(\"</div>\")\n",
    "\n",
    "html_content = \"\\n\".join(html_parts)\n",
    "with open(\"cluster_report.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<html><head><meta charset='utf-8'></head><body>\")\n",
    "    f.write(html_content)\n",
    "    f.write(\"</body></html>\")\n",
    "\n",
    "print(\"HTML report saved to cluster_report.html\")\n",
    "print(\"\\nPipeline complete ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6d36d-621f-40d5-9895-c4bcf32c9f7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d3023-b3ac-4c1d-81ca-500b6661129f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
